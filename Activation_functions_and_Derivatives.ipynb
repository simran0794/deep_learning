{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **ACTIVATION FUNCTIONS AND THEIR DERIVATIVES**"
      ],
      "metadata": {
        "id": "HwtCK50cVfGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Importing necessary libraries\n",
        "* Creating a dataset and then standardizing it.\n",
        "* Splitting the dataset in training and testing set."
      ],
      "metadata": {
        "id": "r6EaQUrNToZK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MMsllVaOvr7o"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
        "y = y.reshape(-1, 1)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Defining the Activation functions and their respective derivatives"
      ],
      "metadata": {
        "id": "NTcdadMJT8YT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def tanh(z):\n",
        "    return np.tanh(z)\n",
        "\n",
        "def relu_derivative(z):\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "def tanh_derivative(z):\n",
        "    return 1 - np.tanh(z)**2\n"
      ],
      "metadata": {
        "id": "iWiOITZ-wQmo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Defining the loss function and Accuracy function"
      ],
      "metadata": {
        "id": "vkkp26zEUSlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cross_entropy_loss(y_true, y_pred):\n",
        "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    return np.mean((y_pred > 0.5) == y_true)\n"
      ],
      "metadata": {
        "id": "5vRItGcJwdDU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Creating the Neural Network"
      ],
      "metadata": {
        "id": "Rm7wptnzUjzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size, activation, activation_derivative, lr=0.01, epochs=1000):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.activation = activation\n",
        "        self.activation_derivative = activation_derivative\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.Z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.A1 = self.activation(self.Z1)\n",
        "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
        "        self.A2 = sigmoid(self.Z2)\n",
        "        return self.A2\n",
        "\n",
        "    def backward(self, X, y, y_pred):\n",
        "        m = X.shape[0]\n",
        "\n",
        "        dZ2 = y_pred - y\n",
        "        dW2 = np.dot(self.A1.T, dZ2) / m\n",
        "        db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "\n",
        "        dZ1 = np.dot(dZ2, self.W2.T) * self.activation_derivative(self.Z1)\n",
        "        dW1 = np.dot(X.T, dZ1) / m\n",
        "        db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "\n",
        "        self.W1 -= self.lr * dW1\n",
        "        self.b1 -= self.lr * db1\n",
        "        self.W2 -= self.lr * dW2\n",
        "        self.b2 -= self.lr * db2\n",
        "\n",
        "    def train(self, X, y):\n",
        "        for epoch in range(self.epochs):\n",
        "            y_pred = self.forward(X)\n",
        "            loss = binary_cross_entropy_loss(y, y_pred)\n",
        "            self.backward(X, y, y_pred)\n",
        "            if (epoch + 1) % 100 == 0:\n",
        "                print(f\"Epoch {epoch + 1}/{self.epochs} - Loss: {loss:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = self.forward(X)\n",
        "        return (y_pred > 0.5).astype(int)\n"
      ],
      "metadata": {
        "id": "Jsfk8XNvwwhn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Training the network"
      ],
      "metadata": {
        "id": "a45y3Z5OVC2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "activation_functions = {\n",
        "    'ReLU': (relu, relu_derivative),\n",
        "    'Sigmoid': (sigmoid, sigmoid_derivative),\n",
        "    'Tanh': (tanh, tanh_derivative)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, (activation, activation_derivative) in activation_functions.items():\n",
        "    print(f\"Training with {name} activation function\")\n",
        "    nn = NeuralNetwork(input_size=10, hidden_size=5, output_size=1, activation=activation, activation_derivative=activation_derivative, lr=0.01, epochs=1000)\n",
        "    nn.train(X_train, y_train)\n",
        "    train_accuracy = accuracy(y_train, nn.predict(X_train))\n",
        "    test_accuracy = accuracy(y_test, nn.predict(X_test))\n",
        "    results[name] = {'train_accuracy': train_accuracy, 'test_accuracy': test_accuracy}\n",
        "\n",
        "for name, result in results.items():\n",
        "    print(f\"{name} - Train Accuracy: {result['train_accuracy']:.4f}, Test Accuracy: {result['test_accuracy']:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Xhr1My8w1BC",
        "outputId": "3581d2c0-905d-4450-8ad3-372ffa4aca86"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with ReLU activation function\n",
            "Epoch 100/1000 - Loss: 0.6929\n",
            "Epoch 200/1000 - Loss: 0.6927\n",
            "Epoch 300/1000 - Loss: 0.6924\n",
            "Epoch 400/1000 - Loss: 0.6921\n",
            "Epoch 500/1000 - Loss: 0.6914\n",
            "Epoch 600/1000 - Loss: 0.6903\n",
            "Epoch 700/1000 - Loss: 0.6882\n",
            "Epoch 800/1000 - Loss: 0.6844\n",
            "Epoch 900/1000 - Loss: 0.6776\n",
            "Epoch 1000/1000 - Loss: 0.6658\n",
            "Training with Sigmoid activation function\n",
            "Epoch 100/1000 - Loss: 0.6927\n",
            "Epoch 200/1000 - Loss: 0.6926\n",
            "Epoch 300/1000 - Loss: 0.6925\n",
            "Epoch 400/1000 - Loss: 0.6924\n",
            "Epoch 500/1000 - Loss: 0.6923\n",
            "Epoch 600/1000 - Loss: 0.6921\n",
            "Epoch 700/1000 - Loss: 0.6919\n",
            "Epoch 800/1000 - Loss: 0.6917\n",
            "Epoch 900/1000 - Loss: 0.6914\n",
            "Epoch 1000/1000 - Loss: 0.6911\n",
            "Training with Tanh activation function\n",
            "Epoch 100/1000 - Loss: 0.6922\n",
            "Epoch 200/1000 - Loss: 0.6906\n",
            "Epoch 300/1000 - Loss: 0.6862\n",
            "Epoch 400/1000 - Loss: 0.6744\n",
            "Epoch 500/1000 - Loss: 0.6475\n",
            "Epoch 600/1000 - Loss: 0.6002\n",
            "Epoch 700/1000 - Loss: 0.5398\n",
            "Epoch 800/1000 - Loss: 0.4812\n",
            "Epoch 900/1000 - Loss: 0.4338\n",
            "Epoch 1000/1000 - Loss: 0.3988\n",
            "ReLU - Train Accuracy: 0.5750, Test Accuracy: 0.5200\n",
            "Sigmoid - Train Accuracy: 0.5150, Test Accuracy: 0.4450\n",
            "Tanh - Train Accuracy: 0.8700, Test Accuracy: 0.8350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ehvGYjyDx3J7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}